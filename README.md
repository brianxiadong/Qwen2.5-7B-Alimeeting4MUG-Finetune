# Qwen2.5-7B AliMeeting4MUG LoRA Fine-tuning

ä½¿ç”¨ **LLaMA-Factory** å¯¹ Qwen2.5-7B è¿›è¡Œ LoRA å¾®è°ƒï¼Œè®­ç»ƒæ¨¡å‹æ‰§è¡Œä¼šè®®ç†è§£ä¸ç”Ÿæˆï¼ˆMUGï¼‰ä»»åŠ¡ã€‚

## ï¿½ å¿«é€Ÿå¼€å§‹

```bash
# 1. å…‹éš†é¡¹ç›®
git clone https://github.com/brianxiadong/Qwen2.5-7B-Alimeeting4MUG-Finetune.git
cd Qwen2.5-7B-Alimeeting4MUG-Finetune

# 2. ä¸€é”®é…ç½®ç¯å¢ƒ (è‡ªåŠ¨åˆ›å»º conda ç¯å¢ƒå¹¶å®‰è£…æ‰€æœ‰ä¾èµ–)
bash setup.sh

# 3. æ¿€æ´»ç¯å¢ƒ
conda activate qwen_finetune

# 4. ä¸‹è½½æ¨¡å‹
python scripts/download_model.py

# 5. é¢„å¤„ç†æ•°æ®
python scripts/preprocess_data.py

# 6. å¼€å§‹è®­ç»ƒ
llamafactory-cli train configs/train_lora.yaml
```

## ï¿½ğŸ“‹ ç›®å½•

- [é¡¹ç›®ç®€ä»‹](#é¡¹ç›®ç®€ä»‹)
- [æ•°æ®é›†è¯´æ˜](#æ•°æ®é›†è¯´æ˜)
- [ç¯å¢ƒé…ç½®](#ç¯å¢ƒé…ç½®)
- [æ¨¡å‹ä¸‹è½½](#æ¨¡å‹ä¸‹è½½)
- [æ•°æ®é¢„å¤„ç†](#æ•°æ®é¢„å¤„ç†)
- [æ¨¡å‹è®­ç»ƒ](#æ¨¡å‹è®­ç»ƒ)
- [æ¨¡å‹æ¨ç†](#æ¨¡å‹æ¨ç†)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®åŸºäºé˜¿é‡Œå·´å·´ **AliMeeting4MUG** æ•°æ®é›†ï¼Œä½¿ç”¨ LoRAï¼ˆLow-Rank Adaptationï¼‰æŠ€æœ¯å¯¹ Qwen2.5-7B å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œé«˜æ•ˆå¾®è°ƒã€‚

### æ”¯æŒçš„ MUG ä»»åŠ¡

| ä»»åŠ¡ | è‹±æ–‡ | è¯´æ˜ |
|------|------|------|
| ä¸»é¢˜æ ‡é¢˜ç”Ÿæˆ | Topic Title Generation (TTG) | ä¸ºä¼šè®®ç‰‡æ®µç”Ÿæˆç®€æ´çš„ä¸»é¢˜æ ‡é¢˜ |
| æŠ½å–å¼æ‘˜è¦ | Extractive Summarization (ES) | ä»ä¼šè®®ä¸­æå–å…³é”®å¥å­ä½œä¸ºæ‘˜è¦ |
| ä¸»é¢˜åˆ†å‰² | Topic Segmentation (TS) | è¯†åˆ«ä¼šè®®ä¸­çš„ä¸»é¢˜è¾¹ç•Œ |
| å…³é”®è¯æå– | Keyphrase Extraction (KPE) | æå–ä¼šè®®å…³é”®è¯ |
| è¡ŒåŠ¨é¡¹æ£€æµ‹ | Action Item Detection (AID) | æ£€æµ‹ä¼šè®®ä¸­çš„å¾…åŠäº‹é¡¹ |

---

## æ•°æ®é›†è¯´æ˜

### æ¦‚è¿°

AliMeeting4MUG æ˜¯é˜¿é‡Œå·´å·´å‘å¸ƒçš„å¤§è§„æ¨¡ä¸­æ–‡ä¼šè®®ç†è§£è¯­æ–™åº“ï¼ŒåŒ…å« 654 åœºå½•åˆ¶çš„æ™®é€šè¯ä¼šè®®ï¼Œæ¯åœºä¼šè®® 15-30 åˆ†é’Ÿï¼Œæ¶‰åŠ 2-4 åå‚ä¸è€…ã€‚

### æ–‡ä»¶ç»“æ„

```
dataset/
â”œâ”€â”€ train.csv    # è®­ç»ƒé›† (296 æ¡ä¼šè®®, ~30MB)
â””â”€â”€ dev.csv      # éªŒè¯é›† (66 æ¡ä¼šè®®, ~7MB)
```

### CSV æ ¼å¼

æ¯ä¸ª CSV æ–‡ä»¶åŒ…å«ä¸¤åˆ—ï¼š

| åˆ—å | è¯´æ˜ |
|------|------|
| `idx` | æ ·æœ¬ç´¢å¼• (0, 1, 2, ...) |
| `content` | JSON æ ¼å¼çš„ä¼šè®®æ•°æ® |

### Content JSON ç»“æ„

```json
{
  "meeting_key": "M0138",
  
  "topic_segment_ids": [
    {
      "id": 88,
      "candidate": [
        {
          "title": "æ–‡è‰ºæ™šä¼šæ‰¾é¢†å¯¼è®²è¯å¹¶å®‰æ’åº§ä½",
          "key_sentence": ["6", "24", "45"]
        },
        {
          "title": "å¦‚ä½•å®‰æ’æ–‡è‰ºæ™šä¼šçš„åº§ä½",
          "key_sentence": ["60", "77"]
        }
      ]
    }
  ],
  
  "sentence_list": [
    {
      "id": 1,
      "speaker": "no.0",
      "start_time": "0.0",
      "end_time": "5.2",
      "s": "ä»Šå¤©æˆ‘ä»¬æ¥è®¨è®ºä¸€ä¸‹æ™šä¼šçš„å®‰æ’ã€‚"
    },
    {
      "id": 2,
      "speaker": "no.1", 
      "start_time": "5.5",
      "end_time": "10.1",
      "s": "å¥½çš„ï¼Œæˆ‘ä»¬å…ˆä»åº§ä½å¼€å§‹ã€‚"
    }
  ],
  
  "paragraph_segment_ids": [
    {"id": 3}, {"id": 10}, {"id": 25}
  ],
  
  "action_ids": [
    {"id": 45}, {"id": 120}
  ]
}
```

### å­—æ®µè¯¦è§£

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `meeting_key` | string | ä¼šè®®å”¯ä¸€æ ‡è¯†ç¬¦ |
| `topic_segment_ids` | array | ä¸»é¢˜åˆ†æ®µä¿¡æ¯ï¼Œæ¯ä¸ªåˆ†æ®µåŒ…å« ID å’Œå€™é€‰æ ‡é¢˜ |
| `topic_segment_ids[].id` | int | è¯¥ä¸»é¢˜æ®µç»“æŸçš„å¥å­ ID |
| `topic_segment_ids[].candidate` | array | å€™é€‰æ ‡é¢˜åˆ—è¡¨ï¼ˆé€šå¸¸ 3 ä¸ªï¼‰ |
| `candidate[].title` | string | ä¸»é¢˜æ ‡é¢˜ |
| `candidate[].key_sentence` | array | è¯¥ä¸»é¢˜çš„å…³é”®å¥å­ ID åˆ—è¡¨ |
| `sentence_list` | array | å®Œæ•´ä¼šè®®è½¬å½• |
| `sentence_list[].id` | int | å¥å­ ID |
| `sentence_list[].speaker` | string | è¯´è¯äººæ ‡è¯† (no.0, no.1, ...) |
| `sentence_list[].start_time` | string | å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰ |
| `sentence_list[].end_time` | string | ç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰ |
| `sentence_list[].s` | string | å¥å­æ–‡æœ¬å†…å®¹ |
| `paragraph_segment_ids` | array | æ®µè½åˆ†æ®µç‚¹çš„å¥å­ ID |
| `action_ids` | array | è¡ŒåŠ¨é¡¹å¥å­çš„ ID |

---

## ç¯å¢ƒé…ç½®

### 1. åˆ›å»º Conda ç¯å¢ƒ

```bash
# åˆ›å»ºæ–°çš„ Python 3.10 ç¯å¢ƒ
conda create -n qwen_finetune python=3.10 -y
conda activate qwen_finetune

# å®‰è£… PyTorch (æ ¹æ® CUDA ç‰ˆæœ¬é€‰æ‹©)
# CUDA 11.8
pip install torch==2.1.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# æˆ– CUDA 12.1
# pip install torch==2.1.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### 2. å…‹éš†é¡¹ç›®å’Œ LLaMA-Factory

```bash
# å…‹éš†æœ¬é¡¹ç›®
git clone https://github.com/brianxiadong/Qwen2.5-7B-Alimeeting4MUG-Finetune.git
cd Qwen2.5-7B-Alimeeting4MUG-Finetune

# å…‹éš† LLaMA-Factory
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e ".[torch,metrics]"
cd ..
```

### 3. å®‰è£…é¡¹ç›®ä¾èµ–

```bash
pip install -r requirements.txt
```

### 4. å®‰è£… Flash Attention 2ï¼ˆæ¨èï¼‰

Flash Attention 2 æ˜¯ä¸€ç§é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶å®ç°ï¼Œå¯ä»¥ï¼š
- âš¡ **åŠ é€Ÿè®­ç»ƒ** 1.5-2 å€
- ğŸ’¾ **å‡å°‘æ˜¾å­˜å ç”¨** 5-20 å€ï¼ˆé’ˆå¯¹æ³¨æ„åŠ›å±‚ï¼‰
- ğŸ“ˆ **æ”¯æŒæ›´é•¿åºåˆ—** è€Œä¸ä¼š OOM

#### æ–¹å¼ä¸€ï¼šä½¿ç”¨é¢„ç¼–è¯‘ wheelï¼ˆæ¨èï¼‰

ç”±äº Flash Attention ç¼–è¯‘å¾ˆæ…¢ï¼Œå»ºè®®ç›´æ¥ä¸‹è½½é¢„ç¼–è¯‘çš„ wheel æ–‡ä»¶ï¼š

```bash
# 1. æ£€æµ‹ä½ çš„ç¯å¢ƒç‰ˆæœ¬
python scripts/check_flash_attn_env.py

# æˆ–ä½¿ç”¨ä¸€è¡Œå‘½ä»¤å¿«é€Ÿæ£€æµ‹
python -c "import torch; import sys; v=sys.version_info; print(f'Python: cp{v.major}{v.minor}, PyTorch: {torch.__version__.split(\"+\")[0]}, CUDA: {torch.version.cuda}, CXX11_ABI: {torch._C._GLIBCXX_USE_CXX11_ABI}')"
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
Python: cp312, PyTorch: 2.5.0, CUDA: 12.1, CXX11_ABI: False
```

2. æ ¹æ®è¾“å‡ºï¼Œåˆ° [Flash Attention Releases](https://github.com/Dao-AILab/flash-attention/releases) ä¸‹è½½å¯¹åº”ç‰ˆæœ¬ï¼š

| ç¯å¢ƒ | wheel æ–‡ä»¶å |
|------|-------------|
| Python 3.12 + PyTorch 2.5 + CUDA 12 + ABI=False | `flash_attn-2.8.3+cu12torch2.5cxx11abiFALSE-cp312-cp312-linux_x86_64.whl` |
| Python 3.12 + PyTorch 2.5 + CUDA 12 + ABI=True | `flash_attn-2.8.3+cu12torch2.5cxx11abiTRUE-cp312-cp312-linux_x86_64.whl` |
| Python 3.10 + PyTorch 2.1 + CUDA 11.8 | `flash_attn-2.8.3+cu118torch2.1cxx11abiFALSE-cp310-cp310-linux_x86_64.whl` |

3. ä¸‹è½½å¹¶å®‰è£…ï¼š
```bash
# ä¸‹è½½ (æ›¿æ¢ä¸ºä½ çš„ç‰ˆæœ¬)
wget https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-xxx.whl

# å®‰è£…
pip install flash_attn-xxx.whl
```

#### æ–¹å¼äºŒï¼šä»æºç ç¼–è¯‘ï¼ˆæ…¢ï¼Œçº¦ 10-30 åˆ†é’Ÿï¼‰

```bash
pip install flash-attn --no-build-isolation
```

> âš ï¸ ç¼–è¯‘éœ€è¦å¤§é‡ RAMï¼ˆå»ºè®® 32GB+ï¼‰å’Œ CUDA å¼€å‘ç¯å¢ƒã€‚

#### éªŒè¯å®‰è£…

```bash
python -c "import flash_attn; print(f'Flash Attention {flash_attn.__version__} installed successfully!')"
```

### 5. å®‰è£… DeepSpeedï¼ˆå¤š GPU è®­ç»ƒï¼‰

```bash
pip install deepspeed
```

### 5. éªŒè¯å®‰è£…

```bash
# éªŒè¯ LLaMA-Factory
llamafactory-cli version

# éªŒè¯ PyTorch CUDA
python -c "import torch; print(f'PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}')"
```

---

## æ¨¡å‹ä¸‹è½½

### ä½¿ç”¨ ModelScope ä¸‹è½½ï¼ˆæ¨èå›½å†…ç”¨æˆ·ï¼‰

```bash
# å®‰è£… modelscope
pip install modelscope

# ä¸‹è½½ Qwen2.5-7B æ¨¡å‹
python scripts/download_model.py --model_id Qwen/Qwen2.5-7B --cache_dir ./models
```

ä¸‹è½½å®Œæˆåï¼Œä¿®æ”¹ `configs/train_lora.yaml` ä¸­çš„æ¨¡å‹è·¯å¾„ï¼š
```yaml
model_name_or_path: ./models/Qwen/Qwen2.5-7B
```

### å…¶ä»–å¯é€‰æ¨¡å‹

| æ¨¡å‹ | ModelScope ID | æ˜¾å­˜éœ€æ±‚ |
|------|---------------|----------|
| Qwen2.5-7B | `Qwen/Qwen2.5-7B` | ~24GB (LoRA) |
| Qwen2.5-7B-Instruct | `Qwen/Qwen2.5-7B-Instruct` | ~24GB (LoRA) |
| Qwen2.5-3B | `Qwen/Qwen2.5-3B` | ~12GB (LoRA) |
| Qwen2.5-1.5B | `Qwen/Qwen2.5-1.5B` | ~8GB (LoRA) |

---

## æ•°æ®é¢„å¤„ç†

### è¿è¡Œé¢„å¤„ç†è„šæœ¬

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd /path/to/Qwen2.5-7B-Alimeeting4MUG-Finetune

# æ‰§è¡Œæ•°æ®è½¬æ¢ï¼ˆé»˜è®¤ï¼šä¸»é¢˜æ ‡é¢˜ç”Ÿæˆä»»åŠ¡ï¼‰
python scripts/preprocess_data.py

# æˆ–æŒ‡å®šå…¶ä»–ä»»åŠ¡
python scripts/preprocess_data.py --task extractive_summary
```

### æ£€æŸ¥è¾“å‡º

```bash
# æŸ¥çœ‹ç”Ÿæˆçš„è®­ç»ƒæ•°æ®
head -n 3 data/train_alpaca.json
```

é¢„æœŸè¾“å‡ºæ ¼å¼ï¼š
```json
[
  {
    "instruction": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¼šè®®åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¼šè®®å†…å®¹ç‰‡æ®µï¼Œç”Ÿæˆä¸€ä¸ªç®€æ´å‡†ç¡®çš„ä¸»é¢˜æ ‡é¢˜ã€‚",
    "input": "ä¼šè®®å†…å®¹ï¼š\n[no.0]: ä»Šå¤©æˆ‘ä»¬æ¥è®¨è®ºä¸€ä¸‹æ™šä¼šçš„å®‰æ’ã€‚\n[no.1]: å¥½çš„ï¼Œæˆ‘ä»¬å…ˆä»åº§ä½å¼€å§‹ã€‚",
    "output": "æ–‡è‰ºæ™šä¼šæ‰¾é¢†å¯¼è®²è¯å¹¶å®‰æ’åº§ä½"
  }
]
```

### å¤åˆ¶æ•°æ®é›†é…ç½®

**é‡è¦**ï¼šéœ€è¦å°† `dataset_info.json` å¤åˆ¶åˆ° LLaMA-Factory çš„ data ç›®å½•ï¼Œæˆ–å°†ç”Ÿæˆçš„æ•°æ®æ–‡ä»¶å¤åˆ¶è¿‡å»ï¼š

```bash
# æ–¹å¼1ï¼šå¤åˆ¶é…ç½®åˆ° LLaMA-Factory
cp data/dataset_info.json /path/to/LLaMA-Factory/data/
cp data/*.json /path/to/LLaMA-Factory/data/

# æ–¹å¼2ï¼šåœ¨é…ç½®ä¸­ä½¿ç”¨ç»å¯¹è·¯å¾„
# ä¿®æ”¹ configs/train_lora.yaml ä¸­çš„ dataset_dir ä¸ºç»å¯¹è·¯å¾„
```

---

## æ¨¡å‹è®­ç»ƒ

### åŸºç¡€è®­ç»ƒå‘½ä»¤

```bash
cd /path/to/LLaMA-Factory

# ä½¿ç”¨é¡¹ç›®é…ç½®æ–‡ä»¶è®­ç»ƒ
llamafactory-cli train /path/to/Qwen2.5-7B-Alimeeting4MUG-Finetune/configs/train_lora.yaml
```

### æ˜¾å­˜ä¸è¶³æ—¶ä½¿ç”¨é‡åŒ–

ç¼–è¾‘ `configs/train_lora.yaml`ï¼Œå–æ¶ˆæ³¨é‡Šé‡åŒ–é…ç½®ï¼š

```yaml
# 4-bit é‡åŒ– (é€‚ç”¨äº 16GB æ˜¾å­˜ GPU)
quantization_bit: 4
quantization_method: bitsandbytes
```

### å¤š GPU è®­ç»ƒ

```bash
# ä½¿ç”¨ DeepSpeed ZeRO-2
CUDA_VISIBLE_DEVICES=0,1,2,3 llamafactory-cli train configs/train_lora.yaml \
    --deepspeed examples/deepspeed/ds_z2_config.json
```

### è®­ç»ƒå‚æ•°è¯´æ˜

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `lora_rank` | 64 | LoRA ç§©ï¼Œè¶Šå¤§è¡¨è¾¾èƒ½åŠ›è¶Šå¼º |
| `lora_alpha` | 128 | LoRA ç¼©æ”¾å› å­ |
| `learning_rate` | 2e-4 | å­¦ä¹ ç‡ |
| `num_train_epochs` | 3 | è®­ç»ƒè½®æ•° |
| `per_device_train_batch_size` | 2 | æ¯ GPU æ‰¹æ¬¡å¤§å° |
| `gradient_accumulation_steps` | 8 | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° |
| `cutoff_len` | 2048 | æœ€å¤§åºåˆ—é•¿åº¦ |

---

## æ¨¡å‹æ¨ç†

### äº¤äº’å¼å¯¹è¯

```bash
cd /path/to/LLaMA-Factory

llamafactory-cli chat /path/to/Qwen2.5-7B-Alimeeting4MUG-Finetune/configs/inference.yaml
```

### ç¤ºä¾‹å¯¹è¯

```
User: è¯·æ ¹æ®ä»¥ä¸‹ä¼šè®®å†…å®¹ç”Ÿæˆä¸»é¢˜æ ‡é¢˜ï¼š
[no.0]: æˆ‘ä»¬æ¥è®¨è®ºä¸€ä¸‹ä¸‹å‘¨çš„äº§å“å‘å¸ƒä¼šã€‚
[no.1]: å‘å¸ƒä¼šçš„åœºåœ°å·²ç»ç¡®å®šäº†å—ï¼Ÿ
[no.0]: ç¡®å®šäº†ï¼Œåœ¨å…¬å¸å¤§ä¼šè®®å®¤ã€‚
[no.1]: å¥½çš„ï¼Œé‚£æˆ‘ä»¬éœ€è¦å‡†å¤‡å“ªäº›ææ–™ï¼Ÿ

Assistant: äº§å“å‘å¸ƒä¼šåœºåœ°åŠææ–™å‡†å¤‡è®¨è®º
```

### åˆå¹¶ LoRA æƒé‡åˆ°åŸºç¡€æ¨¡å‹

```bash
llamafactory-cli export configs/merge.yaml
```

---

## æ¨¡å‹éªŒè¯ä¸è¯„ä¼°

### è®­ç»ƒæŸå¤± (Loss) å‚è€ƒæ ‡å‡†

| é˜¶æ®µ | Loss èŒƒå›´ | è¯´æ˜ |
|------|-----------|------|
| åˆå§‹ | 2.0 - 5.0 | è®­ç»ƒåˆšå¼€å§‹çš„æŸå¤± |
| æ”¶æ•›å | 0.5 - 1.5 | è¾ƒå¥½çš„æ”¶æ•›çŠ¶æ€ |
| ç†æƒ³ç›®æ ‡ | 0.3 - 0.8 | ä¼šè®®ç”Ÿæˆä»»åŠ¡çš„åˆç†èŒƒå›´ |

> **æ³¨æ„**: Loss è¿‡ä½ï¼ˆ< 0.1ï¼‰å¯èƒ½æ„å‘³ç€è¿‡æ‹Ÿåˆï¼Œéœ€æ£€æŸ¥éªŒè¯é›† loss æ˜¯å¦åŒæ­¥ä¸‹é™ã€‚

### æ¨è GPU é…ç½®

| GPU | æ˜¾å­˜ | æ¨èé…ç½® | é¢„ä¼°è®­ç»ƒæ—¶é—´ |
|-----|------|----------|--------------|
| **A800 80GB** | 80GB | batch_size=8, cutoff_len=4096, lora_rank=128 | ~25 åˆ†é’Ÿ |
| **A100 80GB** | 80GB | batch_size=8, cutoff_len=4096, lora_rank=128 | ~25 åˆ†é’Ÿ |
| **A100 40GB** | 40GB | batch_size=2, cutoff_len=2048, lora_rank=64 | ~40 åˆ†é’Ÿ |
| **RTX 4090** | 24GB | batch_size=1, cutoff_len=2048, 4bité‡åŒ– | ~60 åˆ†é’Ÿ |

### éªŒè¯è®­ç»ƒæ•ˆæœ

#### 1. æŸ¥çœ‹è®­ç»ƒæ—¥å¿—

```bash
# æŸ¥çœ‹è®­ç»ƒçŠ¶æ€
cat outputs/qwen2.5-7b-mug-lora/trainer_state.json | python -m json.tool
```

#### 2. è¿è¡Œè¯„ä¼°è„šæœ¬

```bash
# åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹
python scripts/evaluate.py \
    --model_path outputs/qwen2.5-7b-mug-lora \
    --data_path data/dev_alpaca.json \
    --output_path outputs/eval_results.json
```

#### 3. è¯„ä¼°æŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ | è‰¯å¥½èŒƒå›´ |
|------|------|----------|
| ROUGE-L | ç”Ÿæˆæ–‡æœ¬ä¸å‚è€ƒçš„æœ€é•¿å…¬å…±å­åºåˆ— | > 0.4 |
| BLEU-4 | N-gram åŒ¹é…ç²¾åº¦ | > 0.3 |
| Exact Match | å®Œå…¨åŒ¹é…ç‡ | > 0.1 |

### å¸¸è§è®­ç»ƒé—®é¢˜æ’æŸ¥

| é—®é¢˜ | ç°è±¡ | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| Loss ä¸ä¸‹é™ | è®­ç»ƒå¤š epoch å loss ä» > 3.0 | æ£€æŸ¥æ•°æ®æ ¼å¼ã€å¢å¤§å­¦ä¹ ç‡ |
| Loss è¿‡é«˜ | æœ€ç»ˆ loss > 2.0 | å¢åŠ  epochã€å¢å¤§ batch_size |
| è¿‡æ‹Ÿåˆ | è®­ç»ƒ loss é™ä½†éªŒè¯ loss å‡ | å¢åŠ  dropoutã€å‡å°‘ epoch |

---

## å¸¸è§é—®é¢˜

### Q1: æ˜¾å­˜ä¸è¶³ (OOM)

**è§£å†³æ–¹æ¡ˆï¼š**
1. å¯ç”¨ 4-bit é‡åŒ–ï¼šåœ¨ `train_lora.yaml` ä¸­å–æ¶ˆæ³¨é‡Š `quantization_bit: 4`
2. å‡å°æ‰¹æ¬¡å¤§å°ï¼š`per_device_train_batch_size: 1`
3. å‡å°åºåˆ—é•¿åº¦ï¼š`cutoff_len: 1024`
4. ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼š`gradient_checkpointing: true`

### Q2: è®­ç»ƒé€Ÿåº¦æ…¢

**è§£å†³æ–¹æ¡ˆï¼š**
1. å®‰è£… Flash Attention 2ï¼š`pip install flash-attn --no-build-isolation`
2. å¯ç”¨ bf16 è®­ç»ƒï¼ˆéœ€è¦ Ampere åŠä»¥ä¸Š GPUï¼‰
3. ä½¿ç”¨å¤š GPU è®­ç»ƒ

### Q3: æ¨¡å‹è¾“å‡ºè´¨é‡å·®

**è§£å†³æ–¹æ¡ˆï¼š**
1. å¢åŠ è®­ç»ƒè½®æ•°
2. è°ƒæ•´ LoRA rankï¼ˆå°è¯• 128 æˆ– 256ï¼‰
3. æ£€æŸ¥æ•°æ®è´¨é‡ï¼Œç¡®ä¿é¢„å¤„ç†æ­£ç¡®

### Q4: å¦‚ä½•ä½¿ç”¨ Web UI è®­ç»ƒï¼Ÿ

```bash
cd LLaMA-Factory
llamafactory-cli webui
```

ç„¶ååœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ http://localhost:7860

---

## é¡¹ç›®ç»“æ„

```
Qwen2.5-7B-Alimeeting4MUG-Finetune/
â”œâ”€â”€ dataset/                    # åŸå§‹æ•°æ®é›†
â”‚   â”œâ”€â”€ train.csv
â”‚   â””â”€â”€ dev.csv
â”œâ”€â”€ data/                       # å¤„ç†åçš„æ•°æ®
â”‚   â”œâ”€â”€ dataset_info.json       # LLaMA-Factory æ•°æ®é›†é…ç½®
â”‚   â”œâ”€â”€ train_alpaca.json       # è®­ç»ƒæ•°æ® (Alpaca æ ¼å¼)
â”‚   â””â”€â”€ dev_alpaca.json         # éªŒè¯æ•°æ® (Alpaca æ ¼å¼)
â”œâ”€â”€ configs/                    # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ train_lora.yaml         # è®­ç»ƒé…ç½®
â”‚   â””â”€â”€ inference.yaml          # æ¨ç†é…ç½®
â”œâ”€â”€ scripts/                    # è„šæœ¬
â”‚   â””â”€â”€ preprocess_data.py      # æ•°æ®é¢„å¤„ç†
â”œâ”€â”€ outputs/                    # è®­ç»ƒè¾“å‡º (è‡ªåŠ¨ç”Ÿæˆ)
â”‚   â””â”€â”€ qwen2.5-7b-mug-lora/    # LoRA æƒé‡
â””â”€â”€ README.md                   # é¡¹ç›®æ–‡æ¡£
```

---

## å‚è€ƒèµ„æ–™

- [LLaMA-Factory GitHub](https://github.com/hiyouga/LLaMA-Factory)
- [Qwen2.5 æŠ€æœ¯æŠ¥å‘Š](https://qwenlm.github.io/blog/qwen2.5/)
- [AliMeeting4MUG è®ºæ–‡](https://arxiv.org/abs/2302.08466)
- [LoRA è®ºæ–‡](https://arxiv.org/abs/2106.09685)

---

## License

MIT License
