### Model Configuration
model_name_or_path: ./models/Qwen/Qwen2.5-7B
trust_remote_code: true

### Fine-tuning Method
stage: sft
do_train: true
finetuning_type: lora

### LoRA Configuration (防过拟合优化版 v3)
lora_rank: 32              # 降低 rank 减少过拟合
lora_alpha: 64             # alpha = 2 × rank (标准做法)
lora_dropout: 0.15         # 提高 dropout 增强正则化
lora_target: all           # 使用 all 目标

### Dataset Configuration
dataset_dir: ./data
dataset: alimeeting4mug_train
eval_dataset: alimeeting4mug_dev
template: qwen
cutoff_len: 4096
max_samples: 10000
overwrite_cache: true
preprocessing_num_workers: 8

### Training Parameters (保守设置防过拟合)
output_dir: ./outputs/qwen2.5-7b-mug-lora-v3
logging_dir: ./outputs/logs
logging_steps: 5
save_steps: 20             # 更频繁保存，便于早期发现问题
eval_steps: 20             # 更频繁评估
save_total_limit: 10

per_device_train_batch_size: 4   # 适中的 batch size
per_device_eval_batch_size: 4
gradient_accumulation_steps: 4   # 有效 batch size = 16
learning_rate: 3.0e-5            # 更保守的学习率
num_train_epochs: 2.0            # 减少 epochs，防止过拟合
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000

### Evaluation
val_size: 0.0
eval_strategy: steps
load_best_model_at_end: true     # 自动选择最优 checkpoint
metric_for_best_model: eval_loss

### Quantization (disabled for bf16 training)
# quantization_bit: 4
# quantization_method: bitsandbytes

### Flash Attention
flash_attn: fa2
