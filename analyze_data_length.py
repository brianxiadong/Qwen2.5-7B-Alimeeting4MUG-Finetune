#!/usr/bin/env python3
"""
åˆ†æ AliMeeting4MUG è®­ç»ƒæ•°æ®çš„ token é•¿åº¦åˆ†å¸ƒ
"""

import json
import os
from collections import Counter

# å°è¯•å¯¼å…¥ matplotlibï¼Œå¦‚æœæ²¡æœ‰å°±ç”¨æ–‡æœ¬è¾“å‡º
try:
    import matplotlib.pyplot as plt
    import matplotlib
    matplotlib.use('Agg')  # éäº¤äº’å¼åç«¯
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False
    print("è­¦å‘Š: matplotlib æœªå®‰è£…ï¼Œå°†åªè¾“å‡ºæ–‡æœ¬ç»Ÿè®¡")

# å°è¯•å¯¼å…¥ tokenizer
try:
    from transformers import AutoTokenizer
    HAS_TOKENIZER = True
except ImportError:
    HAS_TOKENIZER = False
    print("è­¦å‘Š: transformers æœªå®‰è£…ï¼Œå°†ä½¿ç”¨å­—ç¬¦é•¿åº¦ä¼°ç®—")

def analyze_data(data_path, model_path=None):
    """åˆ†ææ•°æ®é•¿åº¦åˆ†å¸ƒ"""
    
    # åŠ è½½æ•°æ®
    print(f"æ­£åœ¨åŠ è½½æ•°æ®: {data_path}")
    with open(data_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    print(f"æ€»æ ·æœ¬æ•°: {len(data)}")
    
    # è®¡ç®—é•¿åº¦
    lengths = []
    
    if HAS_TOKENIZER and model_path and os.path.exists(model_path):
        print(f"ä½¿ç”¨ tokenizer: {model_path}")
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        
        for i, item in enumerate(data):
            # æ„å»ºå®Œæ•´æ–‡æœ¬ (instruction + input + output)
            text = ""
            if "instruction" in item:
                text += item["instruction"]
            if "input" in item:
                text += item["input"]
            if "output" in item:
                text += item["output"]
            
            tokens = tokenizer.encode(text)
            lengths.append(len(tokens))
            
            if (i + 1) % 500 == 0:
                print(f"å·²å¤„ç† {i+1}/{len(data)} æ ·æœ¬...")
    else:
        print("ä½¿ç”¨å­—ç¬¦é•¿åº¦ä¼°ç®— (çº¦ 1.5 å­—ç¬¦ = 1 token)")
        for item in data:
            text = ""
            if "instruction" in item:
                text += item["instruction"]
            if "input" in item:
                text += item["input"]
            if "output" in item:
                text += item["output"]
            # ä¸­æ–‡å¤§çº¦ 1.5 å­—ç¬¦ = 1 token
            lengths.append(int(len(text) / 1.5))
    
    return lengths

def print_statistics(lengths):
    """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
    lengths_sorted = sorted(lengths)
    n = len(lengths)
    
    print("\n" + "="*60)
    print("ğŸ“Š Token é•¿åº¦ç»Ÿè®¡")
    print("="*60)
    print(f"  æ ·æœ¬æ€»æ•°:     {n}")
    print(f"  æœ€å°é•¿åº¦:     {min(lengths)}")
    print(f"  æœ€å¤§é•¿åº¦:     {max(lengths)}")
    print(f"  å¹³å‡é•¿åº¦:     {sum(lengths)/n:.0f}")
    print(f"  ä¸­ä½æ•°:       {lengths_sorted[n//2]}")
    print(f"  75åˆ†ä½æ•°:     {lengths_sorted[int(n*0.75)]}")
    print(f"  90åˆ†ä½æ•°:     {lengths_sorted[int(n*0.90)]}")
    print(f"  95åˆ†ä½æ•°:     {lengths_sorted[int(n*0.95)]}")
    print(f"  99åˆ†ä½æ•°:     {lengths_sorted[int(n*0.99)]}")
    
    print("\n" + "="*60)
    print("ğŸ“ cutoff_len è¦†ç›–ç‡åˆ†æ")
    print("="*60)
    
    thresholds = [1024, 2048, 4096, 6144, 8192, 16384]
    for t in thresholds:
        count = sum(1 for l in lengths if l <= t)
        pct = count / n * 100
        truncated = sum(1 for l in lengths if l > t)
        status = "âœ…" if pct >= 95 else "âš ï¸" if pct >= 80 else "âŒ"
        print(f"  cutoff_len={t:5d}: {status} è¦†ç›– {pct:5.1f}% ({count}/{n}), æˆªæ–­ {truncated} æ ·æœ¬")
    
    print("\n" + "="*60)
    print("ğŸ“ˆ é•¿åº¦åˆ†å¸ƒç›´æ–¹å›¾ (æ–‡æœ¬ç‰ˆ)")
    print("="*60)
    
    # åˆ›å»ºåŒºé—´
    bins = [(0, 512), (512, 1024), (1024, 2048), (2048, 4096), 
            (4096, 8192), (8192, 16384), (16384, float('inf'))]
    
    for low, high in bins:
        count = sum(1 for l in lengths if low <= l < high)
        pct = count / n * 100
        bar = "â–ˆ" * int(pct / 2)
        label = f"{low}-{high}" if high != float('inf') else f"{low}+"
        print(f"  {label:12s}: {bar:25s} {count:4d} ({pct:4.1f}%)")
    
    return lengths_sorted

def plot_distribution(lengths, output_path="token_length_distribution.png"):
    """ç»˜åˆ¶é•¿åº¦åˆ†å¸ƒå›¾"""
    if not HAS_MATPLOTLIB:
        print("\næ— æ³•ç»˜åˆ¶å›¾è¡¨: matplotlib æœªå®‰è£…")
        print("å®‰è£…å‘½ä»¤: pip install matplotlib")
        return
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # å·¦å›¾: ç›´æ–¹å›¾
    ax1 = axes[0]
    ax1.hist(lengths, bins=50, edgecolor='black', alpha=0.7, color='steelblue')
    ax1.axvline(x=2048, color='orange', linestyle='--', linewidth=2, label='cutoff=2048')
    ax1.axvline(x=4096, color='red', linestyle='--', linewidth=2, label='cutoff=4096')
    ax1.axvline(x=8192, color='purple', linestyle='--', linewidth=2, label='cutoff=8192')
    ax1.set_xlabel('Token Length', fontsize=12)
    ax1.set_ylabel('Count', fontsize=12)
    ax1.set_title('Token Length Distribution', fontsize=14)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # å³å›¾: ç´¯ç§¯åˆ†å¸ƒ
    ax2 = axes[1]
    sorted_lengths = sorted(lengths)
    cumulative = [i/len(lengths)*100 for i in range(1, len(lengths)+1)]
    ax2.plot(sorted_lengths, cumulative, color='steelblue', linewidth=2)
    ax2.axhline(y=95, color='green', linestyle='--', linewidth=1, label='95% coverage')
    ax2.axvline(x=2048, color='orange', linestyle='--', linewidth=2, label='cutoff=2048')
    ax2.axvline(x=4096, color='red', linestyle='--', linewidth=2, label='cutoff=4096')
    ax2.axvline(x=8192, color='purple', linestyle='--', linewidth=2, label='cutoff=8192')
    ax2.set_xlabel('Token Length', fontsize=12)
    ax2.set_ylabel('Cumulative %', fontsize=12)
    ax2.set_title('Cumulative Distribution', fontsize=14)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    ax2.set_ylim(0, 100)
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=150)
    print(f"\nğŸ“Š åˆ†å¸ƒå›¾å·²ä¿å­˜: {output_path}")

def main():
    # é…ç½®è·¯å¾„ - æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹
    data_path = "./data/train_alpaca.json"
    model_path = "/data/Alimeeting4MUG/models/Qwen/Qwen2.5-7B"
    output_img = "./token_length_distribution.png"
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not os.path.exists(data_path):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {data_path}")
        print("è¯·ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬")
        return
    
    # åˆ†ææ•°æ®
    lengths = analyze_data(data_path, model_path)
    
    # æ‰“å°ç»Ÿè®¡
    print_statistics(lengths)
    
    # ç»˜åˆ¶å›¾è¡¨
    plot_distribution(lengths, output_img)
    
    print("\n" + "="*60)
    print("ğŸ’¡ å»ºè®®")
    print("="*60)
    sorted_lengths = sorted(lengths)
    p95 = sorted_lengths[int(len(lengths)*0.95)]
    p99 = sorted_lengths[int(len(lengths)*0.99)]
    
    if p95 <= 2048:
        print("  æ¨è cutoff_len: 2048 (è¦†ç›– 95%+ æ•°æ®)")
    elif p95 <= 4096:
        print("  æ¨è cutoff_len: 4096 (è¦†ç›– 95%+ æ•°æ®)")
    elif p95 <= 8192:
        print("  æ¨è cutoff_len: 8192 (è¦†ç›– 95%+ æ•°æ®)")
    else:
        print(f"  æ•°æ®è¾ƒé•¿ï¼Œ95åˆ†ä½æ•°={p95}ï¼Œå»ºè®®è€ƒè™‘æˆªæ–­æˆ–åˆ†æ®µå¤„ç†")
    
    print(f"  (å½“å‰ 95åˆ†ä½æ•°: {p95}, 99åˆ†ä½æ•°: {p99})")

if __name__ == "__main__":
    main()
