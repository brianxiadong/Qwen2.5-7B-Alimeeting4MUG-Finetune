#!/usr/bin/env python3
"""
Flash Attention ç¯å¢ƒæ£€æµ‹è„šæœ¬
ç”¨äºç¡®å®šæ­£ç¡®çš„ Flash Attention wheel ç‰ˆæœ¬

ä½¿ç”¨æ–¹æ³•:
    python scripts/check_flash_attn_env.py

ç„¶åå» https://github.com/Dao-AILab/flash-attention/releases ä¸‹è½½å¯¹åº”ç‰ˆæœ¬
"""

import sys
import platform

def get_cxx11_abi():
    """æ£€æµ‹ CXX11 ABI è®¾ç½®"""
    try:
        import torch
        return torch._C._GLIBCXX_USE_CXX11_ABI
    except:
        return None

def main():
    print("=" * 60)
    print("ğŸ” Flash Attention ç¯å¢ƒæ£€æµ‹")
    print("=" * 60)
    
    # Python ç‰ˆæœ¬
    v = sys.version_info
    python_tag = f"cp{v.major}{v.minor}"
    print(f"\nğŸ“Œ Python ç‰ˆæœ¬: {v.major}.{v.minor}.{v.micro}")
    print(f"   wheel æ ‡ç­¾: {python_tag}")
    
    # PyTorch ç‰ˆæœ¬
    try:
        import torch
        torch_version = torch.__version__.split('+')[0]
        # æå–ä¸»ç‰ˆæœ¬å· (å¦‚ 2.5.0 -> 2.5)
        torch_major_minor = '.'.join(torch_version.split('.')[:2])
        print(f"\nğŸ“Œ PyTorch ç‰ˆæœ¬: {torch.__version__}")
        print(f"   wheel æ ‡ç­¾: torch{torch_major_minor}")
    except ImportError:
        print("\nâŒ PyTorch æœªå®‰è£…")
        torch_major_minor = None
        return
    
    # CUDA ç‰ˆæœ¬
    if torch.cuda.is_available():
        cuda_version = torch.version.cuda
        cuda_major = cuda_version.split('.')[0]  # å¦‚ 12.1 -> 12
        print(f"\nğŸ“Œ CUDA ç‰ˆæœ¬: {cuda_version}")
        print(f"   wheel æ ‡ç­¾: cu{cuda_major}")
    else:
        print("\nâŒ CUDA ä¸å¯ç”¨")
        cuda_major = None
        return
    
    # CXX11 ABI
    cxx11_abi = get_cxx11_abi()
    if cxx11_abi is not None:
        abi_tag = "TRUE" if cxx11_abi else "FALSE"
        print(f"\nğŸ“Œ CXX11 ABI: {cxx11_abi}")
        print(f"   wheel æ ‡ç­¾: cxx11abi{abi_tag}")
    else:
        abi_tag = "FALSE"  # é»˜è®¤
        print(f"\nâš ï¸ CXX11 ABI: æœªçŸ¥ (é»˜è®¤ä½¿ç”¨ FALSE)")
    
    # ç³»ç»Ÿæ¶æ„
    arch = platform.machine()
    os_name = platform.system().lower()
    print(f"\nğŸ“Œ ç³»ç»Ÿæ¶æ„: {os_name}_{arch}")
    
    # ç”Ÿæˆæ¨èçš„ wheel æ–‡ä»¶å
    print("\n" + "=" * 60)
    print("ğŸ“¦ æ¨èçš„ Flash Attention wheel æ–‡ä»¶å:")
    print("=" * 60)
    
    # Flash Attention wheel å‘½åæ ¼å¼:
    # flash_attn-{version}+cu{cuda}torch{torch}cxx11abi{ABI}-{python}-{python}-{platform}.whl
    wheel_name = f"flash_attn-2.8.3+cu{cuda_major}torch{torch_major_minor}cxx11abi{abi_tag}-{python_tag}-{python_tag}-{os_name}_{arch}.whl"
    
    print(f"\n  {wheel_name}")
    
    print("\n" + "=" * 60)
    print("ğŸ”— ä¸‹è½½é“¾æ¥:")
    print("=" * 60)
    print("\n  https://github.com/Dao-AILab/flash-attention/releases")
    print(f"\n  æœç´¢å…³é”®è¯: cu{cuda_major} torch{torch_major_minor} {python_tag}")
    
    # å¿«é€Ÿå®‰è£…å‘½ä»¤
    print("\n" + "=" * 60)
    print("ğŸ“‹ å®‰è£…å‘½ä»¤ (ä¸‹è½½åè¿è¡Œ):")
    print("=" * 60)
    print(f"\n  pip install {wheel_name}")
    
    # ä¸€è¡Œæ£€æµ‹å‘½ä»¤ï¼ˆæ–¹ä¾¿å¤åˆ¶ï¼‰
    print("\n" + "=" * 60)
    print("ğŸ“‹ å¿«é€Ÿä¸€è¡Œæ£€æµ‹å‘½ä»¤:")
    print("=" * 60)
    print('''
  python -c "import torch; import sys; v=sys.version_info; print(f'Python: cp{v.major}{v.minor}, PyTorch: {torch.__version__.split(\\"+\\")[0]}, CUDA: {torch.version.cuda}, CXX11_ABI: {torch._C._GLIBCXX_USE_CXX11_ABI}')"
''')

if __name__ == "__main__":
    main()
